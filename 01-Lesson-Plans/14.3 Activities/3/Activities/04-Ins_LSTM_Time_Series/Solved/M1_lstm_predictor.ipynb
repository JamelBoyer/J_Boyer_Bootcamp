{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFViD0e6-CLw"
      },
      "source": [
        "# LSTM Stock Predictor Using Closing Prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFMFB1C7-CLy"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "In this section, we will prepare the training and testing data for the LSTM model.\n",
        "\n",
        "We will need to:\n",
        "1. Use the `window_data` function to generate the X and y values for the model.\n",
        "2. Split the data into 70% training and 30% testing\n",
        "3. Apply the MinMaxScaler to the `X` and `y` values\n",
        "4. Reshape the `X_train` and `X_test` data for the model.\n",
        "\n",
        "**Note:** The required input format for the LSTM is:\n",
        "\n",
        "```python\n",
        "reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hyNjsx1X-CLz"
      },
      "outputs": [],
      "source": [
        "# Initial imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UOY-xKd3-CLz"
      },
      "outputs": [],
      "source": [
        "# Set the random seed for reproducibility\n",
        "# Note: This is used for model prototyping, but it is good practice to comment this out and run multiple experiments to evaluate your model.\n",
        "from numpy.random import seed\n",
        "\n",
        "seed(1)\n",
        "from tensorflow import random\n",
        "\n",
        "random.set_seed(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXFj6-dr-CL0"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "In this activity, we will use closing prices from different stocks to make predictions of future closing prices based on the temporal data of each stock."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ZKl0u37Z-CL0",
        "outputId": "5f5b7198-e012-4f5c-b98e-caed43ac33a9"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'stock_data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_45308/1277516892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jamel\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stock_data.csv'"
          ]
        }
      ],
      "source": [
        "# Load csv into DataFrame\n",
        "df = pd.read_csv(\n",
        "    \"stock_data.csv\",\n",
        "    index_col=\"date\",\n",
        "    infer_datetime_format=True,\n",
        "    parse_dates=True,\n",
        ")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo2xbVGZ-CL1"
      },
      "source": [
        "### Creating the Features `X` and Target `y` Data\n",
        "\n",
        "The first step towards preparing the data is to create the input features vectors `X` and the target vector `y`. We will use the `window_data()` function to create these vectors.\n",
        "\n",
        "This function chunks the data up with a rolling window of _X<sub>t</sub> - window_ to predict _X<sub>t</sub>_.\n",
        "\n",
        "The function returns two `numpy` arrays:\n",
        "\n",
        "* `X`: The input features vectors.\n",
        "\n",
        "* `y`: The target vector.\n",
        "\n",
        "The function has the following parameters:\n",
        "\n",
        "* `df`: The original DataFrame with the time series data.\n",
        "\n",
        "* `window`: The window size in days of previous closing prices that will be used for the prediction.\n",
        "\n",
        "* `feature_col_number`: The column number from the original DataFrame where the features are located.\n",
        "\n",
        "* `target_col_number`: The column number from the original DataFrame where the target is located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC2j7xY5-CL1"
      },
      "outputs": [],
      "source": [
        "def window_data(df, window, feature_col_number, target_col_number):\n",
        "    \"\"\"\n",
        "    This function accepts the column number for the features (X) and the target (y).\n",
        "    It chunks the data up with a rolling window of Xt - window to predict Xt.\n",
        "    It returns two numpy arrays of X and y.\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(df) - window):\n",
        "        features = df.iloc[i : (i + window), feature_col_number]\n",
        "        target = df.iloc[(i + window), target_col_number]\n",
        "        X.append(features)\n",
        "        y.append(target)\n",
        "    return np.array(X), np.array(y).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqJHdnYy-CL2"
      },
      "source": [
        "In the forthcoming activities, we will predict closing prices using a `5` days windows of previous _T-Bonds_ closing prices, so that, we will create the `X` and `y` vectors by calling the `window_data` function and defining a window size of `5` and setting the features and target column numbers to `2` (this is the column with the _T-Bonds_ closing prices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpCKOyr2-CL2",
        "outputId": "9f19ef3a-5dd6-4398-d5f1-4c4e95bf7eb0"
      },
      "outputs": [],
      "source": [
        "# Creating the features (X) and target (y) data using the window_data() function.\n",
        "window_size = 5\n",
        "\n",
        "feature_column = 2\n",
        "target_column = 2\n",
        "X, y = window_data(df, window_size, feature_column, target_column)\n",
        "print (f\"X sample values:\\n{X[:5]} \\n\")\n",
        "print (f\"y sample values:\\n{y[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2rDr9T-CL3"
      },
      "source": [
        "### Splitting Data Between Training and Testing Sets\n",
        "\n",
        "To avoid the dataset being randomized, we will manually split the data using array slicing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGSDk8Sb-CL3"
      },
      "outputs": [],
      "source": [
        "# Use 70% of the data for training and the remainder for testing\n",
        "split = int(0.7 * len(X))\n",
        "X_train = X[: split]\n",
        "X_test = X[split:]\n",
        "y_train = y[: split]\n",
        "y_test = y[split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc-wg4GM-CL3"
      },
      "source": [
        "### Scaling Data with `MinMaxScaler`\n",
        "\n",
        "Once the training and test datasets are created, we need to scale the data before training the LSTM model. We will use the `MinMaxScaler` from `sklearn` to scale all values between `0` and `1`.\n",
        "\n",
        "Note that we scale both features and target sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORgIa35z-CL4"
      },
      "outputs": [],
      "source": [
        "# Use the MinMaxScaler to scale data between 0 and 1.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the MinMaxScaler object with the training feature data X_train\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Scale the features training and testing sets\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Fit the MinMaxScaler object with the training target data y_train\n",
        "scaler.fit(y_train)\n",
        "\n",
        "# Scale the target training and testing sets\n",
        "y_train = scaler.transform(y_train)\n",
        "y_test = scaler.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kmPFzS_-CL4"
      },
      "source": [
        "### Reshape Features Data for the LSTM Model\n",
        "\n",
        "The LSTM API from Keras needs to receive the features data as a _vertical vector_, so that we need to reshape the `X` data in the form `reshape((X_train.shape[0], X_train.shape[1], 1))`.\n",
        "\n",
        "Both sets, training, and testing are reshaped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfe1e8Z_-CL4",
        "outputId": "2e4e65bc-53da-46a5-8c64-c1a429b2e5ed"
      },
      "outputs": [],
      "source": [
        "# Reshape the features for the model\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "print (f\"X_train sample values:\\n{X_train[:5]} \\n\")\n",
        "print (f\"X_test sample values:\\n{X_test[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U1bOWun-CL5"
      },
      "source": [
        "---\n",
        "\n",
        "## Build and Train the LSTM RNN\n",
        "\n",
        "In this section, we will design a custom LSTM RNN in Keras and fit (train) it using the training data we defined.\n",
        "\n",
        "We will need to:\n",
        "\n",
        "1. Define the model architecture in Keras.\n",
        "\n",
        "2. Compile the model.\n",
        "\n",
        "3. Fit the model to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrMxPPQj-CL5"
      },
      "source": [
        "### Importing the Keras Modules\n",
        "\n",
        "The LSTM RNN model in Keras uses the `Sequential` model and the `LSTM` layer as we did before. However, there is a new type of layer called `Dropout`.\n",
        "\n",
        "* `Dropout`: Dropout is a regularization technique for reducing overfitting in neural networks. This type of layer applies the dropout technique to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZqlltDm-CL5"
      },
      "outputs": [],
      "source": [
        "# Import required Keras modules\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8taQxBf-CL6"
      },
      "source": [
        "### Defining the LSTM RNN Model Structure\n",
        "\n",
        "To create an LSTM RNN model, we will add `LSTM` layers. The `return_sequences` parameter needs to set to `True` every time we add a new `LSTM` layer, excluding the final layer. The `input_shape` is the number of time steps and the number of indicators\n",
        "\n",
        "After each `LSTM` layer, we add a `Dropout` layer to prevent overfitting. The parameter passed to the `Dropout` layer is the fraction of nodes that will be drop on each epoch, for this demo, we will use a dropout value of `0.2`, it means that on each epoch we will randomly drop `20%` of the units.\n",
        "\n",
        "The number of units in each `LSTM` layers, is equal to the size of the time window, in this demo, we are taking five previous `T-Bons` closing price to predict the next closing price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lc1hiYm-CL6"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM RNN model.\n",
        "model = Sequential()\n",
        "\n",
        "number_units = 5\n",
        "dropout_fraction = 0.2\n",
        "\n",
        "# Layer 1\n",
        "model.add(LSTM(\n",
        "    units=number_units,\n",
        "    return_sequences=True,\n",
        "    input_shape=(X_train.shape[1], 1))\n",
        "    )\n",
        "model.add(Dropout(dropout_fraction))\n",
        "# Layer 2\n",
        "model.add(LSTM(units=number_units, return_sequences=True))\n",
        "model.add(Dropout(dropout_fraction))\n",
        "# Layer 3\n",
        "model.add(LSTM(units=number_units))\n",
        "model.add(Dropout(dropout_fraction))\n",
        "# Output layer\n",
        "model.add(Dense(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cRbHVzv-CL6"
      },
      "source": [
        "### Compiling the LSTM RNN Model\n",
        "\n",
        "We will compile the model, using the `adam` optimizer, as loss function, we will use `mean_square_error` since the value we want to predict is continuous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK_eGCKt-CL6"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiXW_98w-CL7",
        "outputId": "da01fbe2-54cd-4576-b08a-464dbcbf786a"
      },
      "outputs": [],
      "source": [
        "# Summarize the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6iHTvb2-CL7"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "Once the model is defined, we train (fit) the model using `10` epochs. Since we are working with time-series data, it's important to set `shuffle=False` since it's necessary to keep the sequential order of the data.\n",
        "\n",
        "We can experiment with the `batch_size` parameter; however, smaller batch size is recommended; in this demo, we will use a `batch_size=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JeQ9swJ-CL7",
        "outputId": "74e503c0-b05c-422b-b2aa-1703ec194a18"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, shuffle=False, batch_size=1, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdk9FJS_-CL7"
      },
      "source": [
        "---\n",
        "## Model Performance\n",
        "\n",
        "In this section, we will evaluate the model using the test data. \n",
        "\n",
        "We will need to:\n",
        "\n",
        "1. Evaluate the model using the `X_test` and `y_test` data.\n",
        "\n",
        "2. Use the `X_test` data to make predictions.\n",
        "\n",
        "3. Create a DataFrame of real (`y_test`) vs predicted values.\n",
        "\n",
        "4. Plot the Real vs predicted values as a line chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ_mHKGX-CL8"
      },
      "source": [
        "### Evaluate the Model\n",
        "\n",
        "It's time to evaluate our model to assess its performance. We will use the `evaluate` method using the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-r_XbAz-CL8",
        "outputId": "88219c59-1a5f-4c9e-ce90-bb79ebe5a71e"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSs-7fBS-CL8"
      },
      "source": [
        "### Making Predictions\n",
        "\n",
        "We will make some closing price predictions using our brand new LSTM RNN model and our testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrRfbCSm-CL8"
      },
      "outputs": [],
      "source": [
        "# Make some predictions\n",
        "predicted = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDgB99pU-CL9"
      },
      "source": [
        "Since we scaled the original values using the `MinMaxScaler`, we need to recover the original prices to better understand the predictions.\n",
        "\n",
        "We will use the `inverse_transform()` method of the scaler to decode the scaled values to their original scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW8-aoAT-CL9"
      },
      "outputs": [],
      "source": [
        "# Recover the original prices instead of the scaled version\n",
        "predicted_prices = scaler.inverse_transform(predicted)\n",
        "real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7A78919-CL9"
      },
      "source": [
        "### Plotting Predicted Vs. Real Prices\n",
        "\n",
        "To plot the predicted vs. the real values, we will create a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "0s2R19jP-CL9",
        "outputId": "dd02e524-4a73-4e1f-ea37-2edb474e5c3b"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame of Real and Predicted values\n",
        "stocks = pd.DataFrame({\n",
        "    \"Real\": real_prices.ravel(),\n",
        "    \"Predicted\": predicted_prices.ravel()\n",
        "    }, index = df.index[-len(real_prices): ])\n",
        "stocks.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "LJCuV5eS-CL-",
        "outputId": "3a4089d3-7f3e-4ccd-b50b-1268c5a8de25"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'stocks' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_45308/1977448243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot the real vs predicted prices as a line chart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'stocks' is not defined"
          ]
        }
      ],
      "source": [
        "# Plot the real vs predicted prices as a line chart\n",
        "stocks.plot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "M1_lstm_predictor.ipynb",
      "provenance": []
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.11 ('dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "vscode": {
      "interpreter": {
        "hash": "88faf968d59841e7ef8a87304018a28eabc64e8d2716b3df8adebfd562b96072"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
